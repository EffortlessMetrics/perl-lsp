name: CI (Expensive)

on:
  pull_request:
    branches: [ main, master ]
    types: [labeled]
  schedule:
    # Run nightly at 3am UTC
    - cron: '0 3 * * *'
  workflow_dispatch:
    inputs:
      save_baseline:
        description: 'Save results as new baseline'
        required: false
        type: boolean
        default: false

# Cancel in-flight runs on new pushes (saves minutes)
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: read
  issues: write
  pull-requests: write

env:
  CARGO_TERM_COLOR: always

jobs:
  mutation:
    name: Mutation Testing
    if: |
      github.event_name == 'workflow_dispatch' ||
      contains(github.event.pull_request.labels.*.name, 'ci:mutation')
    runs-on: ubuntu-latest
    timeout-minutes: 60
    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@stable
        with: { toolchain: 1.90.0 }
      - uses: Swatinem/rust-cache@v2
        with:
          cache-on-failure: true
      - run: cargo install cargo-mutants --locked || true
      - run: cargo mutants --timeout 60 --no-shuffle || true

  benchmark:
    name: Performance Benchmarks
    timeout-minutes: 45
    if: |
      github.event_name == 'workflow_dispatch' ||
      github.event_name == 'schedule' ||
      contains(github.event.pull_request.labels.*.name, 'ci:bench')
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        submodules: recursive

    - name: Install Rust toolchain
      uses: dtolnay/rust-toolchain@stable

    - name: Cache cargo dependencies
      uses: Swatinem/rust-cache@v2
      with:
        key: ${{ runner.os }}-bench-${{ hashFiles('Cargo.lock') }}
        cache-on-failure: true

    - name: Install just
      run: cargo install just

    - name: Run benchmarks
      run: |
        # Run full benchmark suite
        cargo bench --workspace --locked -- --noplot 2>&1 | tee benchmarks/results/raw-output.txt || true

        # Extract structured results from Criterion
        python3 ./benchmarks/scripts/extract-criterion.py --output benchmarks/results/latest.json

    - name: Compare against baseline
      id: compare
      continue-on-error: true
      run: |
        if [ -f "benchmarks/baselines/v0.9.0.json" ]; then
          ./benchmarks/scripts/compare.sh --fail-on-regression 2>&1 | tee comparison.txt
          echo "REGRESSION=$?" >> $GITHUB_OUTPUT
        else
          echo "No baseline found, skipping comparison"
          echo "REGRESSION=0" >> $GITHUB_OUTPUT
        fi

    - name: Generate benchmark receipt
      run: |
        python3 ./benchmarks/scripts/format-results.py benchmarks/results/latest.json --receipt > benchmark_receipt.txt
        cat benchmark_receipt.txt

    - name: Generate markdown report
      run: |
        python3 ./benchmarks/scripts/format-results.py benchmarks/results/latest.json --markdown > benchmark_report.md

    - name: Generate performance alerts
      id: alert
      continue-on-error: true
      run: |
        if [ -f "benchmarks/baselines/v0.9.0.json" ]; then
          python3 ./benchmarks/scripts/alert.py --format markdown > alert.md
          python3 ./benchmarks/scripts/alert.py --check 2>&1
          echo "CRITICAL=$?" >> $GITHUB_OUTPUT
        else
          echo "No baseline found, skipping alerts"
          echo "CRITICAL=0" >> $GITHUB_OUTPUT
        fi

    - name: Comment on PR (performance alerts)
      if: github.event_name == 'pull_request' && (steps.compare.outputs.REGRESSION == '1' || steps.alert.outputs.CRITICAL != '')
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          let body = '';

          // Use new alert markdown if available
          if (fs.existsSync('alert.md')) {
            body = fs.readFileSync('alert.md', 'utf8');
          } else if (fs.existsSync('comparison.txt')) {
            // Fallback to legacy comparison
            const comparison = fs.readFileSync('comparison.txt', 'utf8');
            body = `## Performance Regression Detected\n\n\`\`\`\n${comparison}\n\`\`\`\n\nPlease review the benchmark results and address any performance regressions.`;
          }

          if (body) {
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            });
          }

    - name: Upload benchmark artifacts
      uses: actions/upload-artifact@v6
      with:
        name: benchmark-results-${{ github.sha }}
        path: |
          benchmarks/results/latest.json
          benchmarks/results/raw-output.txt
          benchmark_receipt.txt
          benchmark_report.md
          comparison.txt
          alert.md
        retention-days: 30

    - name: Store benchmark result (for tracking)
      uses: benchmark-action/github-action-benchmark@v1
      with:
        name: Perl LSP Benchmarks
        tool: 'cargo'
        output-file-path: benchmarks/results/raw-output.txt
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: false
        alert-threshold: '120%'
        comment-on-alert: true
        fail-on-alert: false

  parser-benchmarks:
    name: Parser Benchmarks
    timeout-minutes: 30
    if: |
      github.event_name == 'workflow_dispatch' ||
      github.event_name == 'schedule' ||
      contains(github.event.pull_request.labels.*.name, 'ci:bench')
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        submodules: recursive

    - name: Install Rust toolchain
      uses: dtolnay/rust-toolchain@stable

    - name: Cache cargo dependencies
      uses: Swatinem/rust-cache@v2
      with:
        key: ${{ runner.os }}-bench-parser-${{ hashFiles('Cargo.lock') }}
        cache-on-failure: true

    - name: Run parser benchmarks
      run: |
        echo "# Parser Benchmark Results" > parser_results.md
        echo "" >> parser_results.md
        echo "## Criterion Output" >> parser_results.md
        echo '```' >> parser_results.md
        cargo bench --locked -p perl-parser --bench parser_benchmark -- --warm-up-time 2 --measurement-time 5 2>&1 | tee -a parser_results.md
        echo '```' >> parser_results.md

    - name: Upload parser results
      uses: actions/upload-artifact@v6
      with:
        name: parser-benchmark-results
        path: parser_results.md

  lexer-benchmarks:
    name: Lexer Benchmarks
    timeout-minutes: 30
    if: |
      github.event_name == 'workflow_dispatch' ||
      github.event_name == 'schedule' ||
      contains(github.event.pull_request.labels.*.name, 'ci:bench')
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        submodules: recursive

    - name: Install Rust toolchain
      uses: dtolnay/rust-toolchain@stable

    - name: Cache cargo dependencies
      uses: Swatinem/rust-cache@v2
      with:
        key: ${{ runner.os }}-bench-lexer-${{ hashFiles('Cargo.lock') }}
        cache-on-failure: true

    - name: Run lexer benchmarks
      run: |
        echo "# Lexer Benchmark Results" > lexer_results.md
        echo "" >> lexer_results.md
        echo "## Criterion Output" >> lexer_results.md
        echo '```' >> lexer_results.md
        cargo bench --locked -p perl-lexer --bench lexer_benchmarks -- --warm-up-time 2 --measurement-time 5 2>&1 | tee -a lexer_results.md
        echo '```' >> lexer_results.md

    - name: Upload lexer results
      uses: actions/upload-artifact@v6
      with:
        name: lexer-benchmark-results
        path: lexer_results.md

  lsp-benchmarks:
    name: LSP Benchmarks
    timeout-minutes: 30
    if: |
      github.event_name == 'workflow_dispatch' ||
      github.event_name == 'schedule' ||
      contains(github.event.pull_request.labels.*.name, 'ci:bench')
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        submodules: recursive

    - name: Install Rust toolchain
      uses: dtolnay/rust-toolchain@stable

    - name: Cache cargo dependencies
      uses: Swatinem/rust-cache@v2
      with:
        key: ${{ runner.os }}-bench-lsp-${{ hashFiles('Cargo.lock') }}
        cache-on-failure: true

    - name: Run LSP benchmarks
      run: |
        echo "# LSP Benchmark Results" > lsp_results.md
        echo "" >> lsp_results.md
        echo "## Rope Performance" >> lsp_results.md
        echo '```' >> lsp_results.md
        cargo bench --locked -p perl-lsp --bench rope_performance_benchmark -- --warm-up-time 2 --measurement-time 5 2>&1 | tee -a lsp_results.md
        echo '```' >> lsp_results.md

    - name: Upload LSP results
      uses: actions/upload-artifact@v6
      with:
        name: lsp-benchmark-results
        path: lsp_results.md

  index-benchmarks:
    name: Workspace Index Benchmarks
    timeout-minutes: 30
    if: |
      github.event_name == 'workflow_dispatch' ||
      github.event_name == 'schedule' ||
      contains(github.event.pull_request.labels.*.name, 'ci:bench')
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        submodules: recursive

    - name: Install Rust toolchain
      uses: dtolnay/rust-toolchain@stable

    - name: Cache cargo dependencies
      uses: Swatinem/rust-cache@v2
      with:
        key: ${{ runner.os }}-bench-index-${{ hashFiles('Cargo.lock') }}
        cache-on-failure: true

    - name: Run workspace index benchmarks
      run: |
        echo "# Workspace Index Benchmark Results" > index_results.md
        echo "" >> index_results.md
        echo "## Index Performance" >> index_results.md
        echo '```' >> index_results.md
        cargo bench --locked -p perl-workspace-index --bench workspace_index_benchmark -- --warm-up-time 2 --measurement-time 5 2>&1 | tee -a index_results.md
        echo '```' >> index_results.md

    - name: Upload index results
      uses: actions/upload-artifact@v6
      with:
        name: index-benchmark-results
        path: index_results.md

  memory-profile:
    name: Memory Usage Profile
    timeout-minutes: 30
    if: |
      github.event_name == 'workflow_dispatch' ||
      github.event_name == 'schedule' ||
      contains(github.event.pull_request.labels.*.name, 'ci:bench')
    runs-on: ubuntu-latest
    continue-on-error: true

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        submodules: recursive

    - name: Install Rust toolchain
      uses: dtolnay/rust-toolchain@stable

    - name: Install valgrind
      run: sudo apt-get update && sudo apt-get install -y valgrind

    - name: Cache cargo dependencies
      uses: Swatinem/rust-cache@v2
      with:
        key: ${{ runner.os }}-bench-mem-${{ hashFiles('Cargo.lock') }}
        cache-on-failure: true

    - name: Build release binary
      run: cargo build --release -p perl-parser --example parse_file

    - name: Profile memory usage
      run: |
        echo "# Memory Usage Profile" > memory_report.md
        echo "" >> memory_report.md
        echo "## Test Setup" >> memory_report.md
        echo "" >> memory_report.md

        # Create test file
        cat > /tmp/test.pl << 'EOF'
        package TestModule;
        use strict;
        use warnings;

        sub new { bless {}, shift }
        sub process { my ($self, @data) = @_; return \@data; }

        1;
        EOF

        echo "Test file: /tmp/test.txt ($(wc -c < /tmp/test.pl) bytes)" >> memory_report.md
        echo "" >> memory_report.md

        echo "## Valgrind Analysis" >> memory_report.md
        echo "" >> memory_report.md

        # Run with valgrind if binary exists
        if [ -f "target/release/examples/parse_file" ]; then
          valgrind --tool=massif --massif-out-file=massif.out \
            ./target/release/examples/parse_file /tmp/test.pl 2>&1 || true

          if [ -f "massif.out" ]; then
            echo '```' >> memory_report.md
            ms_print massif.out 2>&1 | head -50 >> memory_report.md || echo "ms_print failed" >> memory_report.md
            echo '```' >> memory_report.md
          else
            echo "Massif output not generated" >> memory_report.md
          fi
        else
          echo "Parser binary not found - skipping memory profiling" >> memory_report.md
        fi

    - name: Upload memory profile
      uses: actions/upload-artifact@v6
      with:
        name: memory-profile
        path: |
          memory_report.md
          massif.out

  summary:
    name: Benchmark Summary
    timeout-minutes: 10
    needs: [benchmark, parser-benchmarks, lexer-benchmarks, lsp-benchmarks, index-benchmarks]
    if: always()
    runs-on: ubuntu-latest

    steps:
    - name: Download all artifacts
      uses: actions/download-artifact@v4
      with:
        path: artifacts

    - name: Create summary
      run: |
        echo "# Benchmark Summary" > summary.md
        echo "" >> summary.md
        echo "Run: ${{ github.run_id }}" >> summary.md
        echo "SHA: ${{ github.sha }}" >> summary.md
        echo "" >> summary.md

        echo "## Job Status" >> summary.md
        echo "| Job | Status |" >> summary.md
        echo "|-----|--------|" >> summary.md
        echo "| Performance Benchmarks | ${{ needs.benchmark.result }} |" >> summary.md
        echo "| Parser Benchmarks | ${{ needs.parser-benchmarks.result }} |" >> summary.md
        echo "| Lexer Benchmarks | ${{ needs.lexer-benchmarks.result }} |" >> summary.md
        echo "| LSP Benchmarks | ${{ needs.lsp-benchmarks.result }} |" >> summary.md
        echo "| Index Benchmarks | ${{ needs.index-benchmarks.result }} |" >> summary.md
        echo "" >> summary.md

        # Include benchmark report if available
        if [ -f "artifacts/benchmark-results-${{ github.sha }}/benchmark_report.md" ]; then
          echo "## Detailed Results" >> summary.md
          cat "artifacts/benchmark-results-${{ github.sha }}/benchmark_report.md" >> summary.md
        fi

    - name: Upload summary
      uses: actions/upload-artifact@v6
      with:
        name: benchmark-summary
        path: summary.md